---
title: "Prediction"
author: "Forrest W. Crawford"
output:
  beamer_presentation: 
    slide_level: 2
    toc: true
  html_document:
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: FALSE
      smooth_scroll: TRUE
    code_folding: hide
    highlight: haddock
    number_sections: FALSE
    df_print: kable
bibliography: ../phmodeling.bib
---

# Prediction 

We have talked about calibrating models to data, making their output and dynamics match what we observe empirically in the real world.  
[example]. 

But models are most useful, and most dangerous, when we use them to generalize about circumstances we have not yet seen.  

This could mean a different time, or different variable inputs.  




## Running models forward 

Suppose you have parameterized a model $y(t|\theta)$ with parameters $\theta$ and calibrated it to some time range $[0,T]$.  Prediction means projecting $y(t)$ for $t>T$. 


Sometimes people call this "out of sample prediction".

When the model holds outside the support of the data, then it works well. 

When the model does not hold, then it doesn't. 




# Example: linear regression



# Polynomial regression 




## Causal inference 

These questions suggest a formal view of out-of-sample prediction.  

What-if scenarios
- What if everyone got the same treatment? 
- What if we implement the program? 
- What if we eliminate the program? 

Crash course in causal inference using models 

You observe some data for outcomes under certain circumstances.  You want to know what _would have happened, had circumstances been different_.  To do this, you need to reason about counterfactual outcomes.  When can you do this using models?  

Suppose you believe a linear model for the potential outcome
$$
Y(x) = \alpha + \beta x + \epsilon
$$
changing x one unit gives a change of $\beta$ in the outcome.  
$$
ATE = Y(1) - Y(0) = \beta
$$
Easy! 


---

### Modeler beware: spurious associations

sources of apparent dependence in variables (see figure) 
causation
reverse causation
confounding
selection




Using evidence from trials, surveillance, and observational studies to calibrate mechanistic models. 

Difficulties: many studies report composite or marginal measures like odds ratios.  How to use these properly? 




Making good decisions 


See appendices of Manski public policy book 
See Basu book 

Borrow some of Gregg's books 




---

## References 












